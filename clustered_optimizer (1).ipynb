{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.cluster import KMeans\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import functools\n",
        "from sklearn.metrics import silhouette_samples , silhouette_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NcQNUJY65ElV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "\n",
        "def model_w_parameters(alpha, beta, gamma, data):\n",
        "    \"\"\"\n",
        "    Apply exponential smoothing with damped trend method.\n",
        "    Returns one-step-ahead forecasts for given data.\n",
        "    \"\"\"\n",
        "    observation = data.values  # Convert to numpy array for speed\n",
        "    levels = [observation[0]]\n",
        "    slopes = [(observation[0] - observation[-1]) / len(observation)]\n",
        "    forecasts = np.zeros(len(observation))\n",
        "\n",
        "    for i, o in enumerate(observation):\n",
        "        level_value = o * alpha + (gamma * slopes[i] + levels[i]) * (1 - alpha)\n",
        "        levels.append(level_value)\n",
        "        slopes.append((level_value - levels[i]) * beta + (1 - beta) * gamma * slopes[i])\n",
        "        forecasts[i] = levels[i] + gamma * slopes[i]\n",
        "\n",
        "    return forecasts, slopes[-1]\n",
        "\n",
        "def calculate_mape(params, dataframe, colnames):\n",
        "    \"\"\"Calculate MAPE for given parameters across all columns\"\"\"\n",
        "    alpha, beta, gamma = params\n",
        "    forecasts_list = []\n",
        "\n",
        "    for colname in colnames:\n",
        "        forecasts, _ = model_w_parameters(alpha, beta, gamma, dataframe[colname])\n",
        "        forecasts_list.append(forecasts)\n",
        "\n",
        "    mapes = []\n",
        "    for i, colname in enumerate(colnames):\n",
        "        actual = dataframe[colname].values\n",
        "        forecast = forecasts_list[i]\n",
        "        # Avoid division by zero\n",
        "        valid_indices = actual != 0\n",
        "        if np.any(valid_indices):\n",
        "            mape = np.mean(np.abs((actual[valid_indices] - forecast[valid_indices]) / actual[valid_indices]) * 100)\n",
        "            mapes.append(mape)\n",
        "\n",
        "    return np.mean(mapes) if mapes else float('inf')\n",
        "\n",
        "def optimizer_in_clusters(dataframe, n_jobs=-1, grid_sizes=(10, 5, 3)):\n",
        "    \"\"\"\n",
        "    Optimized version that uses grid search with adaptive refinement and parallel processing.\n",
        "\n",
        "    Args:\n",
        "        dataframe: Input dataframe with time series columns\n",
        "        n_jobs: Number of parallel jobs to run (-1 for all cores)\n",
        "        grid_sizes: Tuple of grid sizes for each refinement level\n",
        "\n",
        "    Returns:\n",
        "        Optimal alpha, beta, gamma parameters and the minimum MAPE\n",
        "    \"\"\"\n",
        "    colnames = dataframe.columns[1:] if len(dataframe.columns) > 1 else dataframe.columns\n",
        "\n",
        "    # Initial grid search with fewer points\n",
        "    best_params = None\n",
        "    best_mape = float('inf')\n",
        "\n",
        "    # Create parameter combinations for initial grid\n",
        "    n1 = grid_sizes[0]\n",
        "    param_grid = []\n",
        "    for a in np.linspace(0.01, 0.99, n1):\n",
        "        for b in np.linspace(0.01, 0.99, n1):\n",
        "            for g in np.linspace(0.05, 0.95, n1): #gamma restricted olanda burası değişecek.\n",
        "                param_grid.append((a, b, g))\n",
        "\n",
        "    # Initial search with parallel processing\n",
        "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
        "        mape_func = functools.partial(calculate_mape, dataframe=dataframe, colnames=colnames)\n",
        "        results = list(executor.map(mape_func, param_grid))\n",
        "\n",
        "    # Find best result from initial grid\n",
        "    best_idx = np.argmin(results)\n",
        "    best_params = param_grid[best_idx]\n",
        "    best_mape = results[best_idx]\n",
        "\n",
        "    # Refine search around best parameters, using smaller grid size each time\n",
        "    for grid_size in grid_sizes[1:]:\n",
        "        alpha_best, beta_best, gamma_best = best_params\n",
        "        param_grid = []\n",
        "\n",
        "        # Create refined grid around best parameters\n",
        "        alpha_range = np.linspace(max(0.01, alpha_best - 0.05), min(0.99, alpha_best + 0.05), grid_size)\n",
        "        beta_range = np.linspace(max(0.01, beta_best - 0.05), min(0.99, beta_best + 0.05), grid_size)\n",
        "        gamma_range = np.linspace(max(0.05, gamma_best - 0.05), min(0.95, gamma_best + 0.05), grid_size) # gamma restricted olacaksa burası değişecek.\n",
        "\n",
        "        for a in alpha_range:\n",
        "            for b in beta_range:\n",
        "                for g in gamma_range:\n",
        "                    param_grid.append((a, b, g))\n",
        "\n",
        "        # Refined search with parallel processing\n",
        "        with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
        "            mape_func = functools.partial(calculate_mape, dataframe=dataframe, colnames=colnames)\n",
        "            results = list(executor.map(mape_func, param_grid))\n",
        "\n",
        "        # Update best parameters\n",
        "        best_idx = np.argmin(results)\n",
        "        best_params = param_grid[best_idx]\n",
        "        best_mape = results[best_idx]\n",
        "\n",
        "    return best_params[0], best_params[1], best_params[2], best_mape\n",
        "def forecaster_clustered(test_data ,train_data , optimizing_parameters , step = 2 , reason = 'test'):\n",
        "  alp,bet,gam = optimizing_parameters\n",
        "  columns = train_data.columns\n",
        "  slopes = [model_w_parameters(alp,bet,gam ,train_data[column])[1] for column in columns]\n",
        "  dftest = test_data\n",
        "  dftrain = train_data\n",
        "  h = step\n",
        "\n",
        "  if reason == 'test':\n",
        "    h = len(dftest)\n",
        "    d_values = [np.array([gam**k for k in range(1, h + 1) ]) for i in range(len(columns))]\n",
        "    forecasts = [[dftest[d].iloc[0] + slopes[i] * (d_values[i][:p]).sum() for p in range(1,h+1)] for i,d in enumerate(columns)]\n",
        "  else:\n",
        "    d_values = [np.array([gam**k for k in range(1, h + 1) ]) for i in range(len(columns))]\n",
        "    forecasts = [[dftest[d].iloc[-1] + slopes[i] * (d_values[i][:p]).sum() for p in range(1,h+1)] for i,d in enumerate(columns)]\n",
        "  return forecasts\n",
        "def get_cluster_numbers(path):\n",
        "  df = pd.read_excel(path).iloc[:,1:]\n",
        "  dfnew = df.transpose()\n",
        "  dfnew = dfnew.reset_index()\n",
        "  dfnew.columns = ['ZoneId'] + [a for a in df['date']]\n",
        "  dfnew = dfnew.iloc[1:,:]\n",
        "  dfcluster = dfnew\n",
        "  data = dfcluster.iloc[0:,1:]\n",
        "  inertias = []\n",
        "  silhouettes = []\n",
        "  for i in range(2,11):\n",
        "    kmeans = KMeans(n_clusters=i)\n",
        "    kmeans.fit(data)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    labels = kmeans.fit_predict(data)\n",
        "    silhouette_avg = silhouette_score(data , labels)\n",
        "    silhouettes.append(silhouette_avg)\n",
        "  kmean_silhouettes = pd.DataFrame({'num_clusters': [*range(2,11)], 'silhouette_scores':silhouettes})\n",
        "  plt.plot(range(2,11), inertias, marker='o')\n",
        "  plt.title('Elbow method')\n",
        "  plt.xlabel('Number of clusters')\n",
        "  plt.ylabel('Inertia')\n",
        "  plt.show()\n",
        "  cluster_numbers = kmean_silhouettes.sort_values(by='silhouette_scores',ascending=False).iloc[:3,0]\n",
        "  return cluster_numbers\n",
        "def info_gatherer(path , cluster_numbers ):\n",
        "  df = pd.read_excel(path).iloc[:,1:]\n",
        "  dftrain = df.iloc[:14,:]\n",
        "  dftest = df.iloc[14:,:]\n",
        "  dfnew = df.transpose()\n",
        "  dfnew = dfnew.reset_index()\n",
        "  dfnew.columns = ['ZoneId'] + [a for a in df['date']]\n",
        "  dfnew = dfnew.iloc[1:,:]\n",
        "  dfcluster = dfnew\n",
        "  data = dfcluster.iloc[0:,1:]\n",
        "  label_dict = {}\n",
        "  for n in cluster_numbers:\n",
        "\n",
        "    for i in range(n):\n",
        "      key = str(n) +'_clusters'\n",
        "      kmeans = KMeans(n_clusters=n)\n",
        "      kmeans.fit(data)\n",
        "      labels = kmeans.fit_predict(data)\n",
        "      label_dict[key] = labels\n",
        "  zone_names_dict = {}\n",
        "  models = {}\n",
        "  errors = {}\n",
        "  for element in label_dict:\n",
        "    data[element] = label_dict[element]\n",
        "    zone_names_dict[element] = {}\n",
        "    models[element] = {}\n",
        "    errors[element] = {}\n",
        "    for cluster in data[element].unique():\n",
        "      zone_names_dict[element][str(cluster)] = dfcluster[data[element] == cluster]['ZoneId']\n",
        "      models[element][str(cluster)] = optimizer_in_clusters(dftrain[zone_names_dict[element][str(cluster)]],8)\n",
        "      alp , bet , gam , _ = models[element][str(cluster)]\n",
        "      forecastsList = forecaster_clustered(dftest[zone_names_dict[element][str(cluster)]],dftrain[zone_names_dict[element][str(cluster)]],(alp,bet,gam),reason='test')\n",
        "      mape_list = []\n",
        "      mae_list = []\n",
        "      mse_list = []\n",
        "\n",
        "      for i,e in enumerate(zone_names_dict[element][str(cluster)]) :\n",
        "        forecasted_value_column = e+'_forecasted'\n",
        "        error_column = e + '_error_term'\n",
        "        dftest[forecasted_value_column] = forecastsList[i]\n",
        "        dftest[error_column] = dftest[e] - dftest[forecasted_value_column]\n",
        "        mape_list.append(((abs(dftest[error_column]/dftest[e]))*100).mean())\n",
        "        mae_list.append(abs(dftest[error_column]).mean())\n",
        "        mse_list.append((dftest[error_column]**2).mean())\n",
        "      errors[element][str(cluster)] = {'mape_list':mape_list , 'mae_list':mae_list , 'mse_list':mse_list}\n",
        "  return zone_names_dict , models , errors\n",
        "\n",
        "\n",
        "def writer(zone_info , parameter_info , error_info ,root  , ilce ,column_order ):\n",
        "  for elements in zone_info:\n",
        "    dict_of_lists = {'ZoneId': [] , 'alpha' : [] , 'beta': [] , 'gamma': [] , 'train_mape' : [] , 'test_mape' : [] , 'test_mse' :[] , 'test_mae' : [] }\n",
        "    for cluster_number in zone_info[elements]:\n",
        "      for i , zone in enumerate(zone_info[elements][cluster_number]):\n",
        "        dict_of_lists['ZoneId'].append(zone)\n",
        "        dict_of_lists['alpha'].append(parameter_info[elements][cluster_number][0])\n",
        "        dict_of_lists['beta'].append(parameter_info[elements][cluster_number][1])\n",
        "        dict_of_lists['gamma'].append(parameter_info[elements][cluster_number][2])\n",
        "        dict_of_lists['train_mape'].append(parameter_info[elements][cluster_number][3])\n",
        "        dict_of_lists['test_mape'].append(error_info[elements][cluster_number]['mape_list'][i])\n",
        "        dict_of_lists['test_mae'].append(error_info[elements][cluster_number]['mae_list'][i])\n",
        "        dict_of_lists['test_mse'].append(error_info[elements][cluster_number]['mse_list'][i])\n",
        "    df = pd.DataFrame(dict_of_lists)\n",
        "    df = df.set_index('ZoneId')\n",
        "    df = df.loc[column_order]\n",
        "    df.to_excel(root + '/' + ilce + '/' + ilce + '_' + elements + '_errors_params-50_kms.xlsx')\n",
        "def clustered_info_to_excel(izmir_poligon_path , ilce):\n",
        "  path = izmir_poligon_path + '/' + ilce + '/' + ilce + '_meandata-50_kms.xlsx'\n",
        "  path2 = izmir_poligon_path + '/' + ilce + '/' + ilce + '_meandata-50.xlsx'\n",
        "  cluster_numbers = get_cluster_numbers(path)\n",
        "  zones , models , errors = info_gatherer(path2 , cluster_numbers)\n",
        "  writer(zones , models , errors , izmir_poligon_path , ilce , pd.read_excel(path2).iloc[:,2:].columns)"
      ],
      "metadata": {
        "id": "ZgHiUev457U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def read_all_data_50(izmir_poligon_path , ilce):\n",
        "  dftrain = pd.read_excel(izmir_poligon_path +'/' +ilce + '/' + ilce +  '_meandata-50_kms.xlsx').iloc[:14,1:]\n",
        "  dftest = pd.read_excel(izmir_poligon_path + '/' +ilce + '/' + ilce +  '_meandata-50_kms.xlsx').iloc[14:,1:]\n",
        "  df = pd.read_excel(izmir_poligon_path +'/' +ilce + '/' + ilce +  '_meandata-50_kms.xlsx').iloc[:,1:]\n",
        "  return df\n",
        "def get_ilce_list(path):\n",
        "  return os.listdir(path)\n",
        "def concat_df(path):\n",
        "  ilce_list = get_ilce_list(path)\n",
        "  df = pd.DataFrame()\n",
        "  for ilce in ilce_list:\n",
        "    if len(df) == 0 :\n",
        "      df = read_all_data_50(path , ilce)\n",
        "    else:\n",
        "      df = df.merge(read_all_data_50(path, ilce) , on='date')\n",
        "  return df  , df.iloc[:14 , :] , df.iloc[14: , :]"
      ],
      "metadata": {
        "id": "jWD8KcbtcZod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('/content/all_meandata-50.xlsx')"
      ],
      "metadata": {
        "id": "wHcfYHjidHeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "izmir_poligon_path = '/content'\n",
        "ilce = 'all'\n",
        "clustered_info_to_excel(izmir_poligon_path,ilce)"
      ],
      "metadata": {
        "id": "V7GsuckMqucr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}