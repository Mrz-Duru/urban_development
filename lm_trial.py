# -*- coding: utf-8 -*-
"""lm_trial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PW3cM8hHXiM9hBZw3t-MlZF2U9Ceekoh
"""

import pandas as pd
import numpy as np
from scipy import stats
from sklearn import linear_model
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples , silhouette_score

def prepare_your_data(km_data , percentage_data , parameter_data):
  """Note that km_data and percentage_data has to be in same shape."""
  df= km_data.iloc[-4:,:].reset_index().iloc[:,1:]
  df3= percentage_data.iloc[-4:,:].reset_index().iloc[:,1:]
  df2= parameter_data.iloc[:,:4]
  xd = pd.Series([]) #xd result seti yani bir sonraki değişimi gösteren set. Regresyonda y olacak.
  qd = pd.Series([]) #qd oraya ilerleyene kadarki averaj değişimi gösteren set.
  zd = pd.Series([]) #zd term başlangıcındaki built yüzdesini gösterecek.
  td = [] #term'ün birinci yarı ya da ikinci yarı olduğunu gösterecek.
  md = pd.Series([])
  kd = pd.Series([])
  res = pd.Series([])
  t_0 = pd.Series([])
  t_1 = pd.Series([])
  t_2 = pd.Series([])
  zones = []
  for i in df.index:
    all_starter = df.iloc[0].values[1:]
    if i > 2:
      three_step_b4 = df.iloc[i-2].values[1:]
      exact_moment = df.iloc[i].values[1:]
      one_step_b4 = df.iloc[i-1].values[1:]
      two_step_b4 = df.iloc[i-2].values[1:]
      xd = pd.concat([xd , pd.Series(100*(exact_moment - one_step_b4)/one_step_b4)] , axis = 0 , ignore_index=True)
      res =pd.concat([res , pd.Series(exact_moment)],axis = 0, ignore_index=True)
      qd = pd.concat([qd , pd.Series((100*(exact_moment - all_starter) / all_starter) / i )] , axis = 0 , ignore_index = True)
      zd = pd.concat([zd , pd.Series(one_step_b4)] , axis = 0 , ignore_index=True)
      md = pd.concat([md , pd.Series(100*(one_step_b4 - two_step_b4)/two_step_b4)], axis = 0 , ignore_index=True)
      kd = pd.concat([kd , pd.Series(df3.iloc[i-1].values[1:])] , axis = 0 ,ignore_index=True )
      t_0=pd.concat([t_0,pd.Series(three_step_b4)] , axis = 0 , ignore_index=True )
      t_1=pd.concat([t_1,pd.Series(two_step_b4)] , axis = 0 , ignore_index=True )
      zones += list(df2['ZoneId'].values)
      if i % 2 == 0:
        td +=[1]*1530

      else:
        td += [2] * 1530
  the_df = pd.DataFrame({'ZoneId': zones ,'n_th_half_of_year' : td ,'start_percentage_built':kd,'last_change' : md,'two_step_before':t_0, 'one_step_before':t_1, 'starting_point':zd , 'average_change_previous_years':qd , 'results':res  })
  the_df = df2.merge(the_df,on='ZoneId')
  return the_df

def read_all_data_multiregress(root ,file):
  df = pd.read_excel(root+'/' + file).iloc[:,1:]
  return df

def data_for_multiregress(root_path , km_data_file, percentage_data_file , parameters_data_file):
  """root_path: is the path where program will find all the given input_files.
     km_data_file: is the actual name without path of the data file for actual kms. e.g "all_meandata-50_kms.xlsx"
     percentage_data_file: is the actual name without path of the data file for percentage built areas. e.g "all_meandata-50.xlsx"
     parameters_data_file: is the actual name without path of the data file for errors and parameters. e.g "all_3_clusters_errors_params-50_kms.xlsx

     With given inputs function will give the information given in data files as three pandas.DataFrame
     read_all_data, which is another procedure might be used as an helper function for data_for_multiregress function."""
  km_data = read_all_data_multiregress(root_path, km_data_file)
  percentage_data = read_all_data_multiregress(root_path,percentage_data_file)
  parameters_data = pd.read_excel(root_path +'/'+ parameters_data_file).iloc[:,:4]
  parameters_data.columns = ['ZoneId'] + [a for a in parameters_data.columns[1:]]
  return km_data , percentage_data , parameters_data

def training_prep(df , df3 , df2 , outlier= 30000):
  """ It is a code that from the given test data for a timeseries dataframe,
      non-time-related feature creation.

      Inputs:
      df: df refers the main_data that used as the creator of the main featuresin the code.
      df3: It is related the df but in this data instead of using the actual one we are using portion of fullness.
      df2: It is the data frame that parameters given for each observation area minimizing parameters.
      outlier: maks_percentage of change allowed.
      Output:
      x and y for the linear regression."""
  xd = pd.Series([]) #xd result seti yani bir sonraki değişimi gösteren set. Regresyonda y olacak.
  qd = pd.Series([]) #qd oraya ilerleyene kadarki averaj değişimi gösteren set.
  zd = pd.Series([]) #zd term başlangıcındaki built yüzdesini gösterecek.
  td = [] #term'ün birinci yarı ya da ikinci yarı olduğunu gösterecek.
  md = pd.Series([])
  kd = pd.Series([])
  res = pd.Series([])
  t_0 = pd.Series([])
  t_1 = pd.Series([])
  t_2 = pd.Series([])
  c_2 = pd.Series([])
  zones = []
  for i in df.index:
    all_starter = df.iloc[0].values[1:]
    if i > 2:
      three_step_b4 = df.iloc[i-2].values[1:]
      exact_moment = df.iloc[i].values[1:]
      one_step_b4 = df.iloc[i-1].values[1:]
      two_step_b4 = df.iloc[i-2].values[1:]
      xd = pd.concat([xd , pd.Series(100*(exact_moment - one_step_b4)/one_step_b4)] , axis = 0 , ignore_index=True)
      res =pd.concat([res , pd.Series(exact_moment)],axis = 0, ignore_index=True)
      qd = pd.concat([qd , pd.Series((100*(exact_moment - all_starter) / all_starter) / i )] , axis = 0 , ignore_index = True)
      zd = pd.concat([zd , pd.Series(one_step_b4)] , axis = 0 , ignore_index=True)
      md = pd.concat([md , pd.Series(100*(one_step_b4 - two_step_b4)/two_step_b4)], axis = 0 , ignore_index=True)
      kd = pd.concat([kd , pd.Series(df3.iloc[i-1].values[1:])] , axis = 0 ,ignore_index=True )
      t_0=pd.concat([t_0,pd.Series(three_step_b4)] , axis = 0 , ignore_index=True )
      t_1=pd.concat([t_1,pd.Series(two_step_b4)] , axis = 0 , ignore_index=True )
      c_2 = pd.concat([c_2 , pd.Series((one_step_b4 - two_step_b4) / two_step_b4)] , axis = 0 , ignore_index = True)
      zones += list(df2['ZoneId'].values)
      if i % 2 == 0:
        td +=[1]*1530

      else:
        td += [2] * 1530
  lm_df = pd.DataFrame({'ZoneId': zones ,'n_th_half_of_year' : td ,'two_step_change':c_2 ,'start_percentage_built':kd,'last_change' : md,'two_step_before':t_0, 'one_step_before':t_1, 'starting_point':zd , 'average_change_previous_years':qd , 'results':res  })
  lm_df = df2.merge(lm_df , on='ZoneId')
  x = lm_df.iloc[:,1:-1]
  y = lm_df.iloc[:,-1]
  collecting_ind = (100*(y - x['starting_point'] ) / x['starting_point']) < outlier
  x = x[collecting_ind]
  y = y[collecting_ind]
  return x , y

def train_test_seperator(data , index = 5):
  """ This procedure is a procedure for seperating te data in appropriate way
      for this very project.

      Inputs
      data : data is a pd.DataFrame object.
      index: It is an integer for determining the size of the test set.
      Outputs
      dftrain : For linear regression model training data available. pd.DataFrame.
      dftest : For linear regression model test data available. pd.DataFrame.
      """

  dftrain = data.iloc[:(-index ),:].reset_index().iloc[:,1:]
  dftest = data.iloc[(-index ):,].reset_index().iloc[:,1:]
  return dftrain , dftest

def one_step_predictor(kms , percentages ,parameters, model = None , outliers = 50):
  """ This is a predicting procedure for getting one-step predictions for the test-possible data.

      Inputs
      kms: it is a dataframe specified for the km2 data.
      percentages: it is a dataframe specified for percentage fullness of the data.
      parameters: it is a dataframe specified for the exponential smoothing with damped trend parameters.
      model: It is a linear regression model trained by the given kms, percentages type of the data.
      outliers: How much change will be allowed?
      Outputs
      prediction_results: an array object placed each row predictions from the data.
      y_test.values: actual observations from that data.
      or
      str: that express you should specify your model first.
      """

  if model:
    x_test ,y_test = training_prep(kms , percentages , parameters ,outliers)
    prediction_results = model.predict(x_test)
    return prediction_results , y_test.values
  return 'Prepare your model first.'

def errors_calculator(predictions_array , observation_array , error_steps = 1):
  """ This is an error calculator for 3 types of error called ape : absolute percentage error , absolute error , squared error.

      Inputs
      predictions_array: This is the array that demonstrates your predictions for the related observation set.
      observation_array: This is the array that demonstrates actual observions related to prediction set.
      error_steps: 1 or specify, this is the chart that demonstrates how many steps of errors placed for the specified area. This is also serves the later track.

      Outputs:
      pd.DataFrame: columns: error types , rows observation sets. """


  ape = (100*abs(observation_array - predictions_array)/observation_array)
  ae = abs(observation_array - predictions_array)
  se = ((observation_array - predictions_array)**2)
  return pd.DataFrame({str(error_steps) + '_step_ape':ape,str(error_steps) + '_step_ae':ae,str(error_steps) + '_step_se':se})

def errors_data_statistics(errors , agg_which = None):
  """ This is a statistics dataframe creator with the desired agg functions.

      Inputs:
      errors: errors, class dataframe.
      agg_which: desired statistical measures. In a tuple object. e.g. ('max' , 'min' , 'mean' , 'std)

      Outputs:
      pd.DataFrame object: columns with errors data columns and rows statistical features given in the agg_which.

  """
  if agg_which:
    return errors.agg(agg_which)
  return errors

def n_model_creator(n , train_data_x , train_data_y ):
  """ It is creating n different clusters and for each clusters creates different linear regression model.

      Inputs:
      n : cluster quantity.
      train_data_x : x we are going to use to construct model.
      train_data_y : y we are going to use to construct model.

      Output:
      frames dictionary. Includes clusters and for each cluster it holds:
      train_data_x : for that cluster train_data for x.
      train_data_y : for that cluster train_data for y.
      model : model construct specially for cluster.
      score : model R2.
      coefficients : weight of each variable regarding the dependent variable. """

  kmeans = KMeans(n_clusters=n)
  train_data_x = train_data_x.iloc[:,4:]
  kmeans.fit(train_data_x)
  labels = kmeans.fit_predict(train_data_x)
  frames = {}
  for i in np.unique(labels):
    index = labels == i
    regr = linear_model.LinearRegression()
    model = regr.fit(train_data_x[index],train_data_y[index])
    score = regr.score(train_data_x[index],train_data_y[index])
    coefs = regr.coef_
    frames[str(i)] = {'x': train_data_x[index] , 'y':train_data_y[index] ,'model': model  ,'score': score , 'coefficients' : coefs}
  return frames ,kmeans

def starting_point(dftrain , dftest):
  thedf = pd.concat([dftrain.iloc[-3:] , dftest.iloc[0:1,:]]).reset_index().iloc[:,1:]
  return thedf

def predictor(df_percentages , df_kms,df_params ,  model_frame , kmean_model ) :
  dftrain_percentages , dftest_percentages = train_test_seperator(df_percentages)
  dftrain_kms , dftest_kms = train_test_seperator(df_kms)
  kamuran_percentages = pd.concat([dftrain_percentages.iloc[-3:] ,dftest_percentages.iloc[0:1, :]]).reset_index().iloc[:,1:]
  kamuran_kms = pd.concat([dftrain_kms.iloc[-3:] ,dftest_kms.iloc[0:1, :]]).reset_index().iloc[:,1:]
  total_kms = kamuran_kms.iloc[-1,1:].values*100/kamuran_percentages.iloc[-1,1:].values
  kamuran_zones = kamuran_kms.columns
  result_df = pd.DataFrame([])
  for a in range(len(dftest_percentages)):

    print(a)
    kamuran_x , _ = training_prep(kamuran_percentages , kamuran_kms , df_params)
    kamuran_x = kamuran_x.iloc[:,4:]
    predictions = []
    less_than_0 = []
    for iks,e in kamuran_x.iterrows():
      prediction = model_frame[str(kmean_model.predict((e.values).reshape(1,-1))[0])]['model'].predict([e])[0]
      predictions.append(prediction)
      if model_frame[str(kmean_model.predict((e.values).reshape(1,-1))[0])]['model'].predict([e])[0]< 0:
        less_than_0.append((str(kmean_model.predict((e.values).reshape(1,-1))[0]) , e , iks))
    addition_frame_percentages = pd.DataFrame( [[a] + predictions],columns=kamuran_zones)
    predicted_kms = list((np.array(predictions) * total_kms)/100)
    addition_frame_kms = pd.DataFrame([[a]+predicted_kms],columns=kamuran_zones)
    kamuran_kms = pd.concat([kamuran_kms , addition_frame_kms] , axis = 0 , ignore_index = True).iloc[1:].reset_index().iloc[:,1:]
    kamuran_percentages = pd.concat([kamuran_percentages , addition_frame_percentages] , axis = 0 , ignore_index = True).iloc[1:].reset_index().iloc[:,1:]
    result_df = pd.concat([result_df , addition_frame_percentages] , axis = 0 , ignore_index=True)
  return result_df , dftest_percentages

df_kms , df_percentage , df_parameters = data_for_multiregress('/content','all_meandata-50_kms.xlsx','all_meandata-50.xlsx' , 'all_3_clusters_errors_params-50_kms.xlsx')

frames,kmeans = n_model_creator(9,x,y)

predicted_values , observations = predictor(df_percentage , df_kms ,df_parameters , frames ,kmeans)

def kmean_model_iterator(df_percent , df_kmsquare , df_params ,start , end):
  n_mean_error = pd.DataFrame([])
  dftrain_kms , dftest_kms = train_test_seperator(df_kmsquare)
  dftrain_percentages , dftest_percentages = train_test_seperator(df_percent)
  x , y = training_prep(dftrain_percentages , dftrain_kms , df_params)
  for indice in range(start , end):
    frames,kmeans = n_model_creator(indice , x ,y)
    predicted_values , observations = predictor(df_percent , df_kmsquare , df_params , frames , kmeans)
    if indice == start :
      n_mean_error['date'] = observations['date']
    error_df = observations.iloc[:,1:] - predicted_values.iloc[:,1:]
    squared_df = error_df ** 2
    squared_errors = squared_df.agg('mean' , axis = 1)
    error_df_abs = error_df.agg('abs')
    percentage_abs = 100*error_df_abs/observations.iloc[:,1:]
    abs_errors = error_df_abs.agg('mean' , axis = 1)
    percentage_abs_errors = percentage_abs.agg('mean',axis = 1)
    n_mean_error[str(indice) + '_clustered_mae'] = abs_errors
    n_mean_error[str(indice) + '_clustered_mape'] = percentage_abs_errors
    n_mean_error[str(indice) + '_clustered_mse'] = squared_errors
  return n_mean_error

model_errors = kmean_model_iterator(df_percentage , df_kms, df_parameters , 3 , 10)

model_errors.to_excel('/content/regression_n_clustered_model_errors.xlsx')